{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd9e69b-85a0-4e1b-b0ab-4d340a86a67f",
   "metadata": {},
   "source": [
    "# Parte 1: Registro de los archivos en la base de datos. (3 Puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443e928-4cb9-4f41-846d-a3a40bb17b4a",
   "metadata": {},
   "source": [
    "## a) Generar una nueva base de datos con la siguiente nomenclatura: apellido_nombre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f2a77-f310-49f7-9269-6585bbc69d18",
   "metadata": {},
   "source": [
    "La base de datos la creé desde DBeaver, dejo la consulta SQL para crearla desde un SQL:\n",
    "\n",
    "CREATE DATABASE francisco_silva;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed618f-edbd-47c2-a64f-f778d15916fa",
   "metadata": {},
   "source": [
    "## b) Importar en tablas los archivos train_cupid.csv y test_cupid.csv a un motor Postgres, implementando sólo la librería psycopg2.\n",
    "Las tablas deben contener los nombres de las columnas y el total de los registros presente en cada archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c394353f-8e92-4f9c-a246-122e1cedc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4eea0f16-1213-42fc-84da-8c97e60eff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conexión a DBeaver\n",
    "connection = psycopg2.connect(\n",
    "                        dbname = \"francisco_silva\",\n",
    "                        port = \"5432\",\n",
    "                        user = \"postgres\",\n",
    "                        password = \"1234\"\n",
    "                        )\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150ecd26-aa3c-4563-a2f7-7d0d79fb6c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'height', 'virgo', 'taurus', 'scorpio', 'pisces', 'libra', 'leo', 'gemini', 'aries', 'aquarius', 'cancer', 'sagittarius', 'asian', 'hispanic__latin', 'black', 'indian', 'pacific_islander', 'native_american', 'middle_eastern', 'colorado', 'new_york', 'oregon', 'arizona', 'hawaii', 'montana', 'wisconsin', 'virginia', 'spain', 'nevada', 'illinois', 'vietnam', 'ireland', 'louisiana', 'michigan', 'texas', 'united_kingdom', 'massachusetts', 'north_carolina', 'idaho', 'mississippi', 'new_jersey', 'florida', 'minnesota', 'georgia', 'utah', 'washington', 'west_virginia', 'connecticut', 'tennessee', 'rhode_island', 'district_of_columbia', 'canada', 'missouri', 'germany', 'pennsylvania', 'netherlands', 'switzerland', 'mexico', 'ohio', 'agnosticism', 'atheism', 'catholicism', 'buddhism', 'judaism', 'hinduism', 'islam', 'pro_dogs', 'pro_cats', 'spanish', 'chinese', 'french', 'german', 'single', 'seeing_someone', 'available', 'employed', 'income_between_25_50', 'income_between_50_75', 'income_over_75', 'drugs_often', 'drugs_sometimes', 'drinks_not_at_all', 'drinks_often', 'drinks_rarely', 'drinks_socially', 'drinks_very_often', 'orientation_gay', 'orientation_straight', 'sex_m', 'smokes_sometimes', 'smokes_trying_to_quit', 'smokes_when_drinking', 'smokes_yes', 'body_type_overweight', 'body_type_regular', 'education_high_school', 'education_undergrad_university']\n"
     ]
    }
   ],
   "source": [
    "#import el csv para hacer reader\n",
    "import csv\n",
    "\n",
    "#importar train_cupid\n",
    "with open(\"./test_cupid.csv\", \"r\") as file:\n",
    "    reader_cols = csv.reader(file)\n",
    "    header = next(reader_cols)\n",
    "    \n",
    "#reemplazamos los espacios y los \"/\" para que no hayan problemas después\n",
    "header = list(map(lambda x: x.replace(\" \", \"_\").replace(\"/\", \"\"), header))\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ef0ce2-5090-4c2a-bc18-17f9dfbbac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE train_cupid(\n",
    "        age decimal,\n",
    "        height decimal,\n",
    "        virgo decimal,\n",
    "        taurus decimal,\n",
    "        scorpio decimal,\n",
    "        pisces decimal,\n",
    "        libra decimal,\n",
    "        leo decimal,\n",
    "        gemini decimal,\n",
    "        aries decimal,\n",
    "        aquarius decimal,\n",
    "        cancer decimal,\n",
    "        sagittarius decimal,\n",
    "        asian decimal,\n",
    "        hispanic_latin decimal,\n",
    "        black decimal,\n",
    "        indian decimal,\n",
    "        pacific_islander decimal,\n",
    "        native_american decimal,\n",
    "        middle_eastern decimal,\n",
    "        colorado decimal,\n",
    "        new_york decimal,\n",
    "        oregon decimal,\n",
    "        arizona decimal,\n",
    "        hawaii decimal,\n",
    "        montana decimal,\n",
    "        wisconsin decimal,\n",
    "        virginia decimal,\n",
    "        spain decimal,\n",
    "        nevada decimal,\n",
    "        illinois decimal,\n",
    "        vietnam decimal,\n",
    "        ireland decimal,\n",
    "        louisiana decimal,\n",
    "        michigan decimal,\n",
    "        texas decimal,\n",
    "        united_kingdom decimal,\n",
    "        massachusetts decimal,\n",
    "        north_carolina decimal,\n",
    "        idaho decimal,\n",
    "        mississippi decimal,\n",
    "        new_jersey decimal,\n",
    "        florida decimal,\n",
    "        minnesota decimal,\n",
    "        georgia decimal,\n",
    "        utah decimal,\n",
    "        washington decimal,\n",
    "        west_virginia decimal,\n",
    "        connecticut decimal,\n",
    "        tennessee decimal,\n",
    "        rhode_island decimal,\n",
    "        district_of_columbia decimal,\n",
    "        canada decimal,\n",
    "        missouri decimal,\n",
    "        germany decimal,\n",
    "        pennsylvania decimal,\n",
    "        netherlands decimal,\n",
    "        switzerland decimal,\n",
    "        mexico decimal,\n",
    "        ohio decimal,\n",
    "        agnosticism decimal,\n",
    "        atheism decimal,\n",
    "        catholicism decimal,\n",
    "        buddhism decimal,\n",
    "        judaism decimal,\n",
    "        hinduism decimal,\n",
    "        islam decimal,\n",
    "        pro_dogs decimal,\n",
    "        pro_cats decimal,\n",
    "        spanish decimal,\n",
    "        chinese decimal,\n",
    "        french decimal,\n",
    "        german decimal,\n",
    "        single decimal,\n",
    "        seeing_someone decimal,\n",
    "        available decimal,\n",
    "        employed decimal,\n",
    "        income_between_25_50 decimal,\n",
    "        income_between_50_75 decimal,\n",
    "        income_over_75 decimal,\n",
    "        drugs_often decimal,\n",
    "        drugs_sometimes decimal,\n",
    "        drinks_not_at_all decimal,\n",
    "        drinks_often decimal,\n",
    "        drinks_rarely decimal,\n",
    "        drinks_socially decimal,\n",
    "        drinks_very_often decimal,\n",
    "        orientation_gay decimal,\n",
    "        orientation_straight decimal,\n",
    "        sex_m decimal,\n",
    "        smokes_sometimes decimal,\n",
    "        smokes_trying_to_quit decimal,\n",
    "        smokes_when_drinking decimal,\n",
    "        smokes_yes decimal,\n",
    "        body_type_overweight decimal,\n",
    "        body_type_regular decimal,\n",
    "        education_high_school decimal,\n",
    "        education_undergrad_university decimal\n",
    "    );\n",
    "\"\"\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE test_cupid(\n",
    "        age decimal,\n",
    "        height decimal,\n",
    "        virgo decimal,\n",
    "        taurus decimal,\n",
    "        scorpio decimal,\n",
    "        pisces decimal,\n",
    "        libra decimal,\n",
    "        leo decimal,\n",
    "        gemini decimal,\n",
    "        aries decimal,\n",
    "        aquarius decimal,\n",
    "        cancer decimal,\n",
    "        sagittarius decimal,\n",
    "        asian decimal,\n",
    "        hispanic_latin decimal,\n",
    "        black decimal,\n",
    "        indian decimal,\n",
    "        pacific_islander decimal,\n",
    "        native_american decimal,\n",
    "        middle_eastern decimal,\n",
    "        colorado decimal,\n",
    "        new_york decimal,\n",
    "        oregon decimal,\n",
    "        arizona decimal,\n",
    "        hawaii decimal,\n",
    "        montana decimal,\n",
    "        wisconsin decimal,\n",
    "        virginia decimal,\n",
    "        spain decimal,\n",
    "        nevada decimal,\n",
    "        illinois decimal,\n",
    "        vietnam decimal,\n",
    "        ireland decimal,\n",
    "        louisiana decimal,\n",
    "        michigan decimal,\n",
    "        texas decimal,\n",
    "        united_kingdom decimal,\n",
    "        massachusetts decimal,\n",
    "        north_carolina decimal,\n",
    "        idaho decimal,\n",
    "        mississippi decimal,\n",
    "        new_jersey decimal,\n",
    "        florida decimal,\n",
    "        minnesota decimal,\n",
    "        georgia decimal,\n",
    "        utah decimal,\n",
    "        washington decimal,\n",
    "        west_virginia decimal,\n",
    "        connecticut decimal,\n",
    "        tennessee decimal,\n",
    "        rhode_island decimal,\n",
    "        district_of_columbia decimal,\n",
    "        canada decimal,\n",
    "        missouri decimal,\n",
    "        germany decimal,\n",
    "        pennsylvania decimal,\n",
    "        netherlands decimal,\n",
    "        switzerland decimal,\n",
    "        mexico decimal,\n",
    "        ohio decimal,\n",
    "        agnosticism decimal,\n",
    "        atheism decimal,\n",
    "        catholicism decimal,\n",
    "        buddhism decimal,\n",
    "        judaism decimal,\n",
    "        hinduism decimal,\n",
    "        islam decimal,\n",
    "        pro_dogs decimal,\n",
    "        pro_cats decimal,\n",
    "        spanish decimal,\n",
    "        chinese decimal,\n",
    "        french decimal,\n",
    "        german decimal,\n",
    "        single decimal,\n",
    "        seeing_someone decimal,\n",
    "        available decimal,\n",
    "        employed decimal,\n",
    "        income_between_25_50 decimal,\n",
    "        income_between_50_75 decimal,\n",
    "        income_over_75 decimal,\n",
    "        drugs_often decimal,\n",
    "        drugs_sometimes decimal,\n",
    "        drinks_not_at_all decimal,\n",
    "        drinks_often decimal,\n",
    "        drinks_rarely decimal,\n",
    "        drinks_socially decimal,\n",
    "        drinks_very_often decimal,\n",
    "        orientation_gay decimal,\n",
    "        orientation_straight decimal,\n",
    "        sex_m decimal,\n",
    "        smokes_sometimes decimal,\n",
    "        smokes_trying_to_quit decimal,\n",
    "        smokes_when_drinking decimal,\n",
    "        smokes_yes decimal,\n",
    "        body_type_overweight decimal,\n",
    "        body_type_regular decimal,\n",
    "        education_high_school decimal,\n",
    "        education_undergrad_university decimal\n",
    "    );\n",
    "\"\"\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c46fe4-b4b2-4bce-b4e7-dda9be878607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b1f5a7-a46f-4c75-9664-76a8349b117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train_cupid.csv', 'r') as file:\n",
    "    # realizamos la ingesta\n",
    "    reader = csv.reader(file)\n",
    "    # ignoramos la primera fila que corresponde al header\n",
    "    next(reader)\n",
    "    # para cada una de las filas remanentes\n",
    "    eses = '%s, '*97\n",
    "    for row in reader:\n",
    "        # ejecutaremos una orden en el cursor que inserte los datos.\n",
    "        cursor.execute(f\"INSERT INTO train_cupid VALUES ({eses} %s)\", row)\n",
    "        \n",
    "with open('./test_cupid.csv', 'r') as file:\n",
    "    # realizamos la ingesta\n",
    "    reader = csv.reader(file)\n",
    "    # ignoramos la primera fila que corresponde al header\n",
    "    next(reader)\n",
    "    # para cada una de las filas remanentes\n",
    "    eses = '%s, '*97\n",
    "    for row in reader:\n",
    "        # ejecutaremos una orden en el cursor que inserte los datos.\n",
    "        cursor.execute(f\"INSERT INTO test_cupid VALUES ({eses} %s)\", row)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe42f5-8fd2-49be-ac82-79c0e53de63c",
   "metadata": {},
   "source": [
    "# Parte 2: Entrenamiento de modelos (3.5 Puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2dab3f-ea81-4fe4-8433-4ff2a74f2e68",
   "metadata": {},
   "source": [
    "## a) Ingestar la tabla de training mediante psycopg2 para el posterior entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5eb2345-687a-4b6a-a6c4-5be84d1eca32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>virgo</th>\n",
       "      <th>taurus</th>\n",
       "      <th>scorpio</th>\n",
       "      <th>pisces</th>\n",
       "      <th>libra</th>\n",
       "      <th>leo</th>\n",
       "      <th>gemini</th>\n",
       "      <th>aries</th>\n",
       "      <th>...</th>\n",
       "      <th>orientation_straight</th>\n",
       "      <th>sex_m</th>\n",
       "      <th>smokes_sometimes</th>\n",
       "      <th>smokes_trying_to_quit</th>\n",
       "      <th>smokes_when_drinking</th>\n",
       "      <th>smokes_yes</th>\n",
       "      <th>body_type_overweight</th>\n",
       "      <th>body_type_regular</th>\n",
       "      <th>education_high_school</th>\n",
       "      <th>education_undergrad_university</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  age height virgo taurus scorpio pisces libra leo gemini aries  ...  \\\n",
       "0  35   70.0     0      0       0      0     0   0      0     0  ...   \n",
       "1  38   68.0     0      0       0      0     0   0      0     0  ...   \n",
       "2  23   71.0     0      0       0      1     0   0      0     0  ...   \n",
       "3  29   66.0     0      0       0      0     0   0      0     0  ...   \n",
       "4  29   67.0     0      1       0      0     0   0      0     0  ...   \n",
       "\n",
       "  orientation_straight sex_m smokes_sometimes smokes_trying_to_quit  \\\n",
       "0                    1     1                0                     0   \n",
       "1                    1     1                0                     0   \n",
       "2                    1     1                0                     0   \n",
       "3                    1     1                0                     0   \n",
       "4                    1     1                0                     0   \n",
       "\n",
       "  smokes_when_drinking smokes_yes body_type_overweight body_type_regular  \\\n",
       "0                    0          0                    0                 1   \n",
       "1                    0          0                    0                 1   \n",
       "2                    0          0                    0                 1   \n",
       "3                    0          0                    0                 0   \n",
       "4                    0          0                    0                 1   \n",
       "\n",
       "  education_high_school education_undergrad_university  \n",
       "0                     0                              0  \n",
       "1                     0                              0  \n",
       "2                     0                              1  \n",
       "3                     0                              1  \n",
       "4                     0                              1  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# generamos la consulta en el cursor\n",
    "cursor.execute(\"SELECT * FROM train_cupid;\")\n",
    "columnas = cursor.fetchall()\n",
    "# Posteriormente transformamos la consulta en una lista antes deingresarla como un pd.DataFrame\n",
    "train = pd.DataFrame(list(columnas), columns = header)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e98af72-5a03-4e9d-bba5-979652ab8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cambiamos los tipos de datos de las columnas ya que vienen como object\n",
    "for i in train.columns:\n",
    "    train[i] = train[i].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b97ae-ed74-4f7f-a59e-8d4db5f71664",
   "metadata": {},
   "source": [
    "## b) Entrenar los siguientes modelos (sin necesidad de ajustar por hiper parámetros):\n",
    "\n",
    "    ○ GradientBoostingClassifier, AdaBoostClassifer, RandomForestClassifier, SVC, DecisionTreeClassifier, LogisticRegression, BernoulliNB.\n",
    "    ○ Existen tres vectores objetivos a evaluar: single, seeing someone y available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed12400-8842-4286-b9aa-ba1dd4e270c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports de modelos\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "#imports de procesamiento\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95758a90-949c-4529-a99a-1052c67bdec7",
   "metadata": {},
   "source": [
    "**Plan para realizarlo**\n",
    "1) for que recorra la lista de modelos\n",
    "2) for que recorra la lista de vectores objectivos\n",
    "3) El doble for total debe serializar el modelo al final de cada ciclo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "127ec19a-af6e-4b27-9989-d0a41d3f8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos las listas de interés\n",
    "modelos = [GradientBoostingClassifier(),\n",
    "           AdaBoostClassifier(),\n",
    "           RandomForestClassifier(),\n",
    "           SVC(),\n",
    "           DecisionTreeClassifier(),\n",
    "           LogisticRegression(),\n",
    "           BernoulliNB()]\n",
    "vectores_objetivos = ['single',\n",
    "                      'seeing_someone',\n",
    "                      'available']\n",
    "features = train.drop(columns = vectores_objetivos).columns\n",
    "\n",
    "#importamos joblib\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253c69c2-af24-422a-8319-376a49fd1943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier()_single\n",
      "GradientBoostingClassifier()_seeing_someone\n",
      "GradientBoostingClassifier()_available\n",
      "AdaBoostClassifier()_single\n",
      "AdaBoostClassifier()_seeing_someone\n",
      "AdaBoostClassifier()_available\n",
      "RandomForestClassifier()_single\n",
      "RandomForestClassifier()_seeing_someone\n",
      "RandomForestClassifier()_available\n",
      "SVC()_single\n",
      "SVC()_seeing_someone\n",
      "SVC()_available\n",
      "DecisionTreeClassifier()_single\n",
      "DecisionTreeClassifier()_seeing_someone\n",
      "DecisionTreeClassifier()_available\n",
      "LogisticRegression()_single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silva\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()_seeing_someone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silva\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()_available\n",
      "BernoulliNB()_single\n",
      "BernoulliNB()_seeing_someone\n",
      "BernoulliNB()_available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silva\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#creamos el doble for\n",
    "for i in modelos:\n",
    "    for j in vectores_objetivos:\n",
    "        \n",
    "        #nombre_modelo\n",
    "        model_name = f\"{i}_{j}\"\n",
    "        print(model_name)\n",
    "        \n",
    "        #creamos X_train e y_train\n",
    "        X_train, y_train = train[features], train[j]\n",
    "        \n",
    "        #fiteamos\n",
    "        modelo = i.fit(X_train, y_train)\n",
    "        \n",
    "        #serializamos\n",
    "        joblib.dump(model_name, f\"{model_name}.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0d163-9c83-47fb-a422-a88ea223a4e0",
   "metadata": {},
   "source": [
    "# Parte 3: Exportación de predicciones (3.5 Puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39516075-a9d9-48d9-8d30-3d7eee4d8ec9",
   "metadata": {},
   "source": [
    "## a) Ingestar la tabla de testing mediante psycopg2 para la posterior predicción del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f39bc3e-5dbc-4b7a-99d5-c4441d686fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>virgo</th>\n",
       "      <th>taurus</th>\n",
       "      <th>scorpio</th>\n",
       "      <th>pisces</th>\n",
       "      <th>libra</th>\n",
       "      <th>leo</th>\n",
       "      <th>gemini</th>\n",
       "      <th>aries</th>\n",
       "      <th>...</th>\n",
       "      <th>orientation_straight</th>\n",
       "      <th>sex_m</th>\n",
       "      <th>smokes_sometimes</th>\n",
       "      <th>smokes_trying_to_quit</th>\n",
       "      <th>smokes_when_drinking</th>\n",
       "      <th>smokes_yes</th>\n",
       "      <th>body_type_overweight</th>\n",
       "      <th>body_type_regular</th>\n",
       "      <th>education_high_school</th>\n",
       "      <th>education_undergrad_university</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  age height virgo taurus scorpio pisces libra leo gemini aries  ...  \\\n",
       "0  22   75.0     0      0       0      0     0   0      1     0  ...   \n",
       "1  32   65.0     1      0       0      0     0   0      0     0  ...   \n",
       "2  24   67.0     0      0       0      0     0   0      0     0  ...   \n",
       "3  29   62.0     0      1       0      0     0   0      0     0  ...   \n",
       "4  39   65.0     0      0       0      0     0   0      0     0  ...   \n",
       "\n",
       "  orientation_straight sex_m smokes_sometimes smokes_trying_to_quit  \\\n",
       "0                    1     1                1                     0   \n",
       "1                    1     0                0                     0   \n",
       "2                    1     0                0                     0   \n",
       "3                    1     0                0                     0   \n",
       "4                    1     0                0                     0   \n",
       "\n",
       "  smokes_when_drinking smokes_yes body_type_overweight body_type_regular  \\\n",
       "0                    0          0                    0                 0   \n",
       "1                    0          0                    0                 0   \n",
       "2                    1          0                    0                 0   \n",
       "3                    0          0                    0                 1   \n",
       "4                    0          0                    0                 0   \n",
       "\n",
       "  education_high_school education_undergrad_university  \n",
       "0                     0                              1  \n",
       "1                     0                              1  \n",
       "2                     0                              1  \n",
       "3                     0                              1  \n",
       "4                     0                              1  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * FROM test_cupid;\")\n",
    "columnas = cursor.fetchall()\n",
    "test = pd.DataFrame(list(columnas), columns = header)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eeecb6f-48fa-4d53-8127-f9148bb5cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cambiamos los tipos de datos de las columnas ya que vienen como object\n",
    "for i in test.columns:\n",
    "    test[i] = test[i].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506b4d7-f49d-42d9-8b66-15201624ab44",
   "metadata": {},
   "source": [
    "## b) En base a los objetos serializados, predecir y evaluar cuatro queries específicas:\n",
    "    ○ Query 1: 'atheism', 'asian', 'employed', 'pro_dogs', 'chinese'.\n",
    "    ○ Query 2: 'income_over_75', 'french', 'german','orientation_straight', 'new_york'.\n",
    "    ○ Query 3: 'education_undergrad_university', 'body_type_regular', 'pro_dogs', 'employed'.\n",
    "    ○ Query 4: 'taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism'.\n",
    "\n",
    "● Cada una de estas queries específicas debe ser registrada en la base de datos.\n",
    "\n",
    "● La base de datos creada debe contener las tablas:\n",
    "\n",
    "    ○ 2 que representan a training y testing.\n",
    "    ○ 84 que representan a cada una de las combinaciones entre modelo, vector y\n",
    "    query específica.\n",
    "\n",
    "● A modo de referencia, la base de datos creada debe contener 86 tablas en total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da9a291-8f79-4e11-b734-6e4264b14ac3",
   "metadata": {},
   "source": [
    "## **Comentario importante:**\n",
    "\n",
    "Dado que los objetos serializados de la parte 2 usan todas las columnas disponibles (menos los 3 vectores objetivos) para entrenarse, no podemos usarlos para esta parte 3 (dado que predeciremos en base a sólo ciertas variables), por tanto generaremos nuevos modelos en base a las querys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14fdbdf-c2af-4246-a8ee-9746c1a6f346",
   "metadata": {},
   "source": [
    "**Plan de trabajo:**\n",
    "\n",
    "    1) USAREMOS el mismo listado de vectores objetivos anteriores.\n",
    "    2) USAREMOS el mismo listado de modelos anteriores.\n",
    "    3) Generaremos una lista que contendrá a su vez una lista por cada listado(valga la redundancia) de features requeridos en cada query.\n",
    "    4) A través de un triple for que hará lo siguiente:\n",
    "        a) filtramos tanto el dataframe TRAIN como TEST para que tenga sólo las columnas de cada una de las 4 querys en 3).\n",
    "        b) para cada query de TRAIN generaremos un modelo con cada uno de los 7 modelos enlistados en 2).\n",
    "        c) los cuales a su vez generarán un modelo para cada vector objetivo enlistado en 1), dando un total así de 7x3x4 modelos (84 en total).\n",
    "        d) serializaremos cada modelo en una carpeta aparte (por tanto los modelos serializados de la parte 2 estarán separados de los de la parte 3).\n",
    "        e) luego, con cada modelo haremos una predicción en base a sus datos respectivos de la base de TEST y guardaremos tanto los datos de TEST como la predicción en un dataframe.\n",
    "        f) finalmente, cada dataframe será subido a la base de datos francisco_silva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "905cdea3-92ae-45eb-83fd-d038192fac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de listas\n",
    "querys = [['atheism', 'asian', 'employed', 'pro_dogs', 'chinese'],\n",
    "          ['income_over_75', 'french', 'german','orientation_straight', 'new_york'],\n",
    "          ['education_undergrad_university', 'body_type_regular', 'pro_dogs', 'employed'],\n",
    "          ['taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism']\n",
    "         ]\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:1234@localhost:5432/francisco_silva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8a49f763-e372-4694-952f-55dbb261430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query1_GradientBoostingClassifier()_single\n",
      "query1_GradientBoostingClassifier()_seeing_someone\n",
      "query1_GradientBoostingClassifier()_available\n",
      "query1_AdaBoostClassifier()_single\n",
      "query1_AdaBoostClassifier()_seeing_someone\n",
      "query1_AdaBoostClassifier()_available\n",
      "query1_RandomForestClassifier()_single\n",
      "query1_RandomForestClassifier()_seeing_someone\n",
      "query1_RandomForestClassifier()_available\n",
      "query1_SVC()_single\n",
      "query1_SVC()_seeing_someone\n",
      "query1_SVC()_available\n",
      "query1_DecisionTreeClassifier()_single\n",
      "query1_DecisionTreeClassifier()_seeing_someone\n",
      "query1_DecisionTreeClassifier()_available\n",
      "query1_LogisticRegression()_single\n",
      "query1_LogisticRegression()_seeing_someone\n",
      "query1_LogisticRegression()_available\n",
      "query1_BernoulliNB()_single\n",
      "query1_BernoulliNB()_seeing_someone\n",
      "query1_BernoulliNB()_available\n",
      "query2_GradientBoostingClassifier()_single\n",
      "query2_GradientBoostingClassifier()_seeing_someone\n",
      "query2_GradientBoostingClassifier()_available\n",
      "query2_AdaBoostClassifier()_single\n",
      "query2_AdaBoostClassifier()_seeing_someone\n",
      "query2_AdaBoostClassifier()_available\n",
      "query2_RandomForestClassifier()_single\n",
      "query2_RandomForestClassifier()_seeing_someone\n",
      "query2_RandomForestClassifier()_available\n",
      "query2_SVC()_single\n",
      "query2_SVC()_seeing_someone\n",
      "query2_SVC()_available\n",
      "query2_DecisionTreeClassifier()_single\n",
      "query2_DecisionTreeClassifier()_seeing_someone\n",
      "query2_DecisionTreeClassifier()_available\n",
      "query2_LogisticRegression()_single\n",
      "query2_LogisticRegression()_seeing_someone\n",
      "query2_LogisticRegression()_available\n",
      "query2_BernoulliNB()_single\n",
      "query2_BernoulliNB()_seeing_someone\n",
      "query2_BernoulliNB()_available\n",
      "query3_GradientBoostingClassifier()_single\n",
      "query3_GradientBoostingClassifier()_seeing_someone\n",
      "query3_GradientBoostingClassifier()_available\n",
      "query3_AdaBoostClassifier()_single\n",
      "query3_AdaBoostClassifier()_seeing_someone\n",
      "query3_AdaBoostClassifier()_available\n",
      "query3_RandomForestClassifier()_single\n",
      "query3_RandomForestClassifier()_seeing_someone\n",
      "query3_RandomForestClassifier()_available\n",
      "query3_SVC()_single\n",
      "query3_SVC()_seeing_someone\n",
      "query3_SVC()_available\n",
      "query3_DecisionTreeClassifier()_single\n",
      "query3_DecisionTreeClassifier()_seeing_someone\n",
      "query3_DecisionTreeClassifier()_available\n",
      "query3_LogisticRegression()_single\n",
      "query3_LogisticRegression()_seeing_someone\n",
      "query3_LogisticRegression()_available\n",
      "query3_BernoulliNB()_single\n",
      "query3_BernoulliNB()_seeing_someone\n",
      "query3_BernoulliNB()_available\n",
      "query4_GradientBoostingClassifier()_single\n",
      "query4_GradientBoostingClassifier()_seeing_someone\n",
      "query4_GradientBoostingClassifier()_available\n",
      "query4_AdaBoostClassifier()_single\n",
      "query4_AdaBoostClassifier()_seeing_someone\n",
      "query4_AdaBoostClassifier()_available\n",
      "query4_RandomForestClassifier()_single\n",
      "query4_RandomForestClassifier()_seeing_someone\n",
      "query4_RandomForestClassifier()_available\n",
      "query4_SVC()_single\n",
      "query4_SVC()_seeing_someone\n",
      "query4_SVC()_available\n",
      "query4_DecisionTreeClassifier()_single\n",
      "query4_DecisionTreeClassifier()_seeing_someone\n",
      "query4_DecisionTreeClassifier()_available\n",
      "query4_LogisticRegression()_single\n",
      "query4_LogisticRegression()_seeing_someone\n",
      "query4_LogisticRegression()_available\n",
      "query4_BernoulliNB()_single\n",
      "query4_BernoulliNB()_seeing_someone\n",
      "query4_BernoulliNB()_available\n"
     ]
    }
   ],
   "source": [
    "for n,i in enumerate(querys):\n",
    "    for j in modelos:\n",
    "        for k in vectores_objetivos:\n",
    "            \n",
    "            #nombre_modelo\n",
    "            model_name = f\"query{n+1}_{j}_{k}\"\n",
    "            print(model_name)\n",
    "\n",
    "            #hacemos el filtro para tener las columnas según la query correspondiente, tanto en train como test\n",
    "            X_train, y_train = train[i], train[k]\n",
    "            X_test = test[i]\n",
    "\n",
    "            #fiteamos\n",
    "            modelo = j.fit(X_train, y_train)\n",
    "            \n",
    "            #predecimos, guardamos y creamos el dataframe a subir a la base de datos\n",
    "            predicciones = pd.DataFrame(modelo.predict(X_test), columns = [k])\n",
    "            df_temp = pd.concat([X_test, predicciones], axis = 1)\n",
    "            \n",
    "            #creamos la base de datos del ciclo\n",
    "            # Construimos las tablas\n",
    "            model_name = model_name.replace('(','').replace(')','')\n",
    "            \n",
    "            text = f\"CREATE TABLE {model_name} (id SERIAL PRIMARY KEY\"\n",
    "\n",
    "            for p, col in enumerate(i):\n",
    "                text += f\",  {col} decimal NOT NULL\"\n",
    "            text += \");\"\n",
    "            cursor.execute(text)\n",
    "            \n",
    "            #subimos df_temp a la base de datos\n",
    "            df_temp.to_sql(model_name, engine)\n",
    "\n",
    "            #serializamos\n",
    "            joblib.dump(model_name, f\"{model_name}.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7670bd4d-7059-4617-b7b4-98996d94a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
