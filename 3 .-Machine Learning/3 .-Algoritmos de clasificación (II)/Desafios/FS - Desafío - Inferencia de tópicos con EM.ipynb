{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_DHHaO4QLAE"
   },
   "source": [
    "# Desafío - Inferencia de tópicos con EM\n",
    "\n",
    "Requerimientos\n",
    "- En esta sesión trabajaremos con una serie de base de datos sobre letras musicales de distintos artistas. Cada uno de los ​csv​ se encuentra en la carpeta ​dump​.\n",
    "- Cada ​csv tiene el nombre del artista a analizar. Los archivos contienen el nombre del artista, el género musical del artista, el nombre de la canción y las letras.\n",
    "- En base a esta información, el objetivo del ejercicio es generar un modelo probabilístico que pueda identificar el género musical más probable dado la letra de una canción.\n",
    "- Para ello implementaremos un modelo conocido como Latent Dirichlet Allocation que hace uso de una variante del algoritmo EM para inferir clases latentes a partir de una matriz de documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZ9tj8DIRLly"
   },
   "source": [
    "# Ejercicio 1: Preparar el ambiente de trabajo\n",
    "\n",
    "- Importe los módulos ​numpy​, ​pandas​, ​matplotlib​, ​seaborn​, ​glob y ​os siguiendo las buenas prácticas. Los últimos dos módulos permitirán realizar la importación de múltiples archivos dentro de la carpeta ​dump​.\n",
    "- Para ello genere un objeto que guarde en una lista todos los archivos alojados en ​dump utilizando ​glob.glob y ​os.getcwd() para extraer las rutas absolutas. Posteriormente genere un objeto ​pd.DataFrame​ que contenga todos los ​csv​.\n",
    "- Asegúrese de eliminar la columna ​Unnamed: ​0​ que se genera por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3486,
     "status": "ok",
     "timestamp": 1634486462684,
     "user": {
      "displayName": "Francisco Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnBCIyF5R8AiTi5ykZU8rl4YlVuugPZqCU5uBJ4YQ=s64",
      "userId": "14439525103954735478"
     },
     "user_tz": 180
    },
    "id": "DR-bleiLQHpm",
    "outputId": "0efdd207-a612-4283-b577-609c54bbda81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-autotime in c:\\users\\silva\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: ipython in c:\\users\\silva\\anaconda3\\lib\\site-packages (from ipython-autotime) (7.22.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\silva\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (0.17.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\silva\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (5.0.9)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\silva\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (52.0.0.post20210125)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\silva\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (3.0.17)\n",
      "Requirement already satisfied: backcall in c:\\users\\silva\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\silva\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (0.4.4)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\silva\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: pygments in c:\\users\\silva\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (2.9.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\silva\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (5.0.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\silva\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\silva\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\silva\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 1.78 s (started: 2021-10-17 17:25:59 -03:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "executionInfo": {
     "elapsed": 12268,
     "status": "ok",
     "timestamp": 1634486474944,
     "user": {
      "displayName": "Francisco Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnBCIyF5R8AiTi5ykZU8rl4YlVuugPZqCU5uBJ4YQ=s64",
      "userId": "14439525103954735478"
     },
     "user_tz": 180
    },
    "id": "BAGe3BDoT13h",
    "outputId": "dc9f84d8-3194-4dd7-f064-dec17b9c49b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artista</th>\n",
       "      <th>Género</th>\n",
       "      <th>Canción</th>\n",
       "      <th>Letra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Deathrider</td>\n",
       "      <td>Riding hard, high in the saddle \\n Winged stee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Metal Thrashing Mad</td>\n",
       "      <td>Racing down the road \\n In a street machine of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>I'm Eighteen</td>\n",
       "      <td>Lines form on my face and hands \\n Lines form ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Panic</td>\n",
       "      <td>Move it to the front \\n Reaching for the light...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Subjugator</td>\n",
       "      <td>Out in the streets \\n We're fighting tonight \\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Artista Género              Canción  \\\n",
       "0  Anthrax  metal           Deathrider   \n",
       "1  Anthrax  metal  Metal Thrashing Mad   \n",
       "2  Anthrax  metal         I'm Eighteen   \n",
       "3  Anthrax  metal                Panic   \n",
       "4  Anthrax  metal           Subjugator   \n",
       "\n",
       "                                               Letra  \n",
       "0  Riding hard, high in the saddle \\n Winged stee...  \n",
       "1  Racing down the road \\n In a street machine of...  \n",
       "2  Lines form on my face and hands \\n Lines form ...  \n",
       "3  Move it to the front \\n Reaching for the light...  \n",
       "4  Out in the streets \\n We're fighting tonight \\...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 359 ms (started: 2021-10-17 17:26:01 -03:00)\n"
     ]
    }
   ],
   "source": [
    "path_colab = \"/content/drive/MyDrive/Data Scientist/Desafío Latam/3 .-Machine Learning/3 .-Algoritmos de clasificación (II)/Desafíos/dump/*\"\n",
    "path_casa = \"dump/*\"\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for name in glob.glob(path_casa):\n",
    "    df = df.append(pd.read_csv(name))\n",
    "\n",
    "df = df.drop(columns = 'Unnamed: 0').rename(columns = {\"0\":\"Artista\", \"1\":\"Género\",\"2\":\"Canción\",\"3\":\"Letra\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6H7XwBXRURQ"
   },
   "source": [
    "# Ejercicio 2: Matriz de ocurrencias\n",
    "\n",
    "- Importe la clase ​CountVectorizer dentro de los módulos ​feature_extraction.text de la librería ​sklearn​.\n",
    "- Aplique la clase para extraer las 5000 palabras más repetidas en toda la base de datos.\n",
    "- Con la clase inicializada, incorpore las letras con el método ​fit_transform y guarde los resultados en un nuevo objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2755,
     "status": "ok",
     "timestamp": 1634486477696,
     "user": {
      "displayName": "Francisco Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnBCIyF5R8AiTi5ykZU8rl4YlVuugPZqCU5uBJ4YQ=s64",
      "userId": "14439525103954735478"
     },
     "user_tz": 180
    },
    "id": "TgqGWjwBRcjf",
    "outputId": "0f8a148b-93ed-4ae2-abdc-9de494eb3fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.5 s (started: 2021-10-17 17:26:01 -03:00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#inicialize\n",
    "count_vectorizer = CountVectorizer(lowercase = True, max_features = 5000, stop_words = 'english')\n",
    "\n",
    "#fiteo\n",
    "count_vectorizer_fit = count_vectorizer.fit_transform(df['Letra'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSqnKdzqRdFY"
   },
   "source": [
    "# Ejercicio 3: Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqKUIMkfRjIl"
   },
   "source": [
    "## a) Importe ​sklearn.decomposition.LatentDirichletAllocation y sklearn.model_selection.GridSearchCV​."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1634486478024,
     "user": {
      "displayName": "Francisco Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnBCIyF5R8AiTi5ykZU8rl4YlVuugPZqCU5uBJ4YQ=s64",
      "userId": "14439525103954735478"
     },
     "user_tz": 180
    },
    "id": "PcYm9lLrRilG",
    "outputId": "5473db7f-cd8f-4816-94c9-036b1c28b95b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 109 ms (started: 2021-10-17 17:26:03 -03:00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r6f3keZRl3F"
   },
   "source": [
    "## b) Genere una búsqueda de grilla con los siguientes hiperparámetros:\n",
    "    ○ n_components: [5, 10, 15].\n",
    "    ○ learning_decay: [0.7, 0.5].\n",
    "\n",
    "**y Entrene la búsqueda de grilla con las letras en un formato vectorizado con CountVectorizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2048913,
     "status": "ok",
     "timestamp": 1634488526936,
     "user": {
      "displayName": "Francisco Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnBCIyF5R8AiTi5ykZU8rl4YlVuugPZqCU5uBJ4YQ=s64",
      "userId": "14439525103954735478"
     },
     "user_tz": 180
    },
    "id": "xTQDj3EbRowg",
    "outputId": "28ece903-89f2-44cb-f30a-c571e45c9638"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LatentDirichletAllocation(),\n",
       "             param_grid={'learning_decay': [0.7, 0.5],\n",
       "                         'n_components': [5, 10, 15]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18min 18s (started: 2021-10-17 17:26:09 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#Definimos los parámetros y los valores a buscar en la grilla\n",
    "parameters = {'n_components':[5,10,15], 'learning_decay':[0.7, 0.5]}\n",
    "\n",
    "#Creamos nuestro X\n",
    "X = df['Letra']\n",
    "\n",
    "#instanciamos la grilla con el modelo y parameters\n",
    "model_lda = GridSearchCV(LatentDirichletAllocation(), parameters).fit(count_vectorizer_fit)\n",
    "model_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPIgQTqURthJ"
   },
   "source": [
    "## c) Reporte brevemente cuál es la mejor combinación de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1634488526936,
     "user": {
      "displayName": "Francisco Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnBCIyF5R8AiTi5ykZU8rl4YlVuugPZqCU5uBJ4YQ=s64",
      "userId": "14439525103954735478"
     },
     "user_tz": 180
    },
    "id": "j1uGoZazRv7K",
    "outputId": "a1c2552a-d510-4f54-84db-add945ec72bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_decay': 0.5, 'n_components': 5}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-10-17 18:16:43 -03:00)\n"
     ]
    }
   ],
   "source": [
    "model_lda.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1856390.0389317055"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-10-17 18:19:24 -03:00)\n"
     ]
    }
   ],
   "source": [
    "model_lda.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czBBFpzc-pKc"
   },
   "source": [
    "**Comentario**\n",
    "\n",
    "La mejor combinación es con learning_decay = 0.5 y con n_components = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP_jmSfFRwTJ"
   },
   "source": [
    "Digresión: Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocatio (LDA) es un modelo probabilístico generativo basado en Inferencia Variacional EM. La principal utilidad de esto es la identificación de tópicos en un corpus de texto. El proceso de inferencia se puede resumir en los siguientes pasos:\n",
    "\n",
    "- Cada documento dentro del corpus se puede entender como una mezcla de tópicos\n",
    "comunes a nivel de corpus​.\n",
    "- Esta mezcla de tópicos es latente: sólo observamos los documentos registrados y sus palabras.\n",
    "- La API de ​sklearn.decomposition.LatentDirichletAllocation presenta la misma\n",
    "funcionalidad de todo modelo de sklearn. Algunos puntos a considerar en la\n",
    "inicialización de la clase son:\n",
    "\n",
    "    ○ n_components​: Cantidad de tópicos a inferir en un corpus.\n",
    "\n",
    "    ○ learning_method​: Forma en la que entran los datos en entrenamiento. Cuando es 'batch', se ingresa la matriz de entrenamiento completa. Cuando es 'online', la matriz de entrenamiento ingresa de manera secuencial en parcelas pequeñas.\n",
    "\n",
    "    ○ l​earning_decay​: Tasa de aprendizaje en la función de pérdida. Cuando se\n",
    "    implementa con ​learning_method=​'online'​, el modelo se entrena con\n",
    "    Gradiente Estocástico Descendente.\n",
    "    \n",
    "    ○ Perplejidad​: Busca aproximar el número óptimo de tópicos a inferir.\n",
    "    Técnicamente evalúa qué tan bien predice una muestra específica. En función a un número de tópicos, define la distribución teórica de palabras representada por los tópicos y la compara con la ocurrencia empírica de palabras en tópicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf7c0xZySPWK"
   },
   "source": [
    "# Ejercicio 4 : Inferencia e Identificación de Tópicos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNfL-v83SR_I"
   },
   "source": [
    "# a) En base a la mejor combinación de hiperparámetros, entrene el modelo con la matriz de atributos de las letras\n",
    "\n",
    "Para identificar de qué se trata cada tópico, necesitamos identificar las principales 15 palabras asociadas con éste. Puede implementar la siguiente línea de código para identificar las principales palabras en un tópico:\n",
    "\n",
    "    # mediante .components_ podemos extraer una matriz que entrega las distribución de palabras por cada tópico.\n",
    "    for topic_id, topic_name in enumerate(fit_best_lda.components_):\n",
    "    # para cada tópico\n",
    "    print(\"tópico: {}\".format(topic_id + 1))\n",
    "    # mediante argsort logramos ordenar los elementos por magnitud\n",
    "    # para los elementos más relevantes ordenados por argsort, buscamos su correlativo\n",
    "    # en la matriz dispersa y devolvemos el nombre.\n",
    "    # finalmente concatenamos las palabras\n",
    "    print(\" \".join([counter.get_feature_names()[i] for i in topic_name.argsort()[:-15 - 1: -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1634488526937,
     "user": {
      "displayName": "Francisco Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnBCIyF5R8AiTi5ykZU8rl4YlVuugPZqCU5uBJ4YQ=s64",
      "userId": "14439525103954735478"
     },
     "user_tz": 180
    },
    "id": "CBRcf5ugSPHS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 41.5 s (started: 2021-10-17 20:02:44 -03:00)\n"
     ]
    }
   ],
   "source": [
    "best_lda = LatentDirichletAllocation(n_components=5, learning_decay=0.5)\n",
    "fit_best_lda = best_lda.fit(count_vectorizer_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tópico: 1\n",
      "don just know ll love ve let baby want come time got like way make\n",
      "tópico: 2\n",
      "oh yeah love hey la know just baby ooh don whoa say like man said\n",
      "tópico: 3\n",
      "life god world death die blood dead eyes time man black light soul like ll\n",
      "tópico: 4\n",
      "like shit got don yo ain fuck nigga em man know just cause niggas bitch\n",
      "tópico: 5\n",
      "got like don know just ah rock cause wanna right ain people ya yeah everybody\n",
      "time: 141 ms (started: 2021-10-17 18:49:57 -03:00)\n"
     ]
    }
   ],
   "source": [
    "# mediante .components_ podemos extraer una matriz que entrega las distribución de palabras por cada tópico.\n",
    "for topic_id, topic_name in enumerate(fit_best_lda.components_):\n",
    "    # para cada tópico\n",
    "    print(\"tópico: {}\".format(topic_id + 1))\n",
    "    # mediante argsort logramos ordenar los elementos por magnitud \n",
    "    # para los elementos más relevantes ordenados por argsort, buscamos su correlativo\n",
    "    # en la matriz dispersa y devolvemos el nombre.\n",
    "    # finalmente concatenamos las palabras\n",
    "    print(\" \".join([count_vectorizer.get_feature_names()[i] for i in topic_name.argsort()[:-15 - 1: -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxrdbtcCSj50"
   },
   "source": [
    "## b) Comente a qué tópicos está asociada cada clase inferida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['metal', 'hiphop', 'rock', 'pop'], dtype=object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-10-17 19:13:12 -03:00)\n"
     ]
    }
   ],
   "source": [
    "df['Género'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentario**\n",
    "\n",
    "Bueno, si tuviera que adivinar...... serían:\n",
    "\n",
    "* Tópico 1: Pop?\n",
    "* Tópico 2: Pop\n",
    "* Tópico 3: Metal\n",
    "* Tópico 4: Hiphop\n",
    "* Tópico 5: Rock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RAoEj5NSmIc"
   },
   "source": [
    "# Ejercicio 5: Identificación de probabilidades\n",
    "- En base a la información generada, es posible identificar cuales van a ser los géneros más probables de ocurrir para un artista.\n",
    "-  Para ello necesitamos guardar la probabilidad de cada canción en nuestra base de datos original. Podemos implementar esto de la siguiente manera:\n",
    "\n",
    "        # generamos una transformación de los datos a distribución de tópico por palabra en el documento\n",
    "        fit_best_lda = best_lda.transform(transformed_feats)\n",
    "\n",
    "        # estra transformación la podemos coercionar a un dataframe de la siguiente manera:        ​\n",
    "        topics_for_each_doc = pd.DataFrame(\n",
    "        ​# pasamos esta matriz y la redondeamos en 3 decimales\n",
    "        np.round(fit_best_lda, ​3​),\n",
    "        ​# agregamos un índice\n",
    "        index=df_lyrics.index\n",
    "        )\n",
    "        \n",
    "        # agregamos identificadores de columna\n",
    "        ​topics_for_each_doc.columns = list(map(​lambda​ x: ​\"T: {}\"​.format(x),\n",
    "        range(​1​, best_lda.n_components + ​1​)))\n",
    "        ​\n",
    "        # concatenamos las probabilidades de tópico por documento a nuestra matriz original\n",
    "        ​concatenated_df = pd.concat([df_lyrics, topics_for_each_doc], axis=​1​)\n",
    "        \n",
    "        # argmax en la matriz de tópicos\n",
    "        concatenated_df[​'highest_topic'​] = np.argmax(docs_topics.values, axis=​1​) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xurSy5yTIkE"
   },
   "source": [
    "## a) Genere una matriz de correlaciones entre la probabilidad de tópicos inferidos. Comente brevemente cuales son las principales asociaciones existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-10-17 20:06:45 -03:00)\n"
     ]
    }
   ],
   "source": [
    "docs_topics = ['T: 1', 'T: 2', 'T: 3', 'T: 4', 'T: 5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1634488526937,
     "user": {
      "displayName": "Francisco Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnBCIyF5R8AiTi5ykZU8rl4YlVuugPZqCU5uBJ4YQ=s64",
      "userId": "14439525103954735478"
     },
     "user_tz": 180
    },
    "id": "dUjJyhQ2TF6z"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artista</th>\n",
       "      <th>Género</th>\n",
       "      <th>Canción</th>\n",
       "      <th>Letra</th>\n",
       "      <th>T: 1</th>\n",
       "      <th>T: 2</th>\n",
       "      <th>T: 3</th>\n",
       "      <th>T: 4</th>\n",
       "      <th>T: 5</th>\n",
       "      <th>highest_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Deathrider</td>\n",
       "      <td>Riding hard, high in the saddle \\n Winged stee...</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.006</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Metal Thrashing Mad</td>\n",
       "      <td>Racing down the road \\n In a street machine of...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>I'm Eighteen</td>\n",
       "      <td>Lines form on my face and hands \\n Lines form ...</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Panic</td>\n",
       "      <td>Move it to the front \\n Reaching for the light...</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anthrax</td>\n",
       "      <td>metal</td>\n",
       "      <td>Subjugator</td>\n",
       "      <td>Out in the streets \\n We're fighting tonight \\...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.003</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Artista Género              Canción  \\\n",
       "0  Anthrax  metal           Deathrider   \n",
       "1  Anthrax  metal  Metal Thrashing Mad   \n",
       "2  Anthrax  metal         I'm Eighteen   \n",
       "3  Anthrax  metal                Panic   \n",
       "4  Anthrax  metal           Subjugator   \n",
       "\n",
       "                                               Letra   T: 1   T: 2   T: 3  \\\n",
       "0  Riding hard, high in the saddle \\n Winged stee...  0.006  0.006  0.888   \n",
       "1  Racing down the road \\n In a street machine of...  0.005  0.307  0.678   \n",
       "2  Lines form on my face and hands \\n Lines form ...  0.645  0.002  0.002   \n",
       "3  Move it to the front \\n Reaching for the light...  0.133  0.399  0.462   \n",
       "4  Out in the streets \\n We're fighting tonight \\...  0.003  0.003  0.855   \n",
       "\n",
       "    T: 4   T: 5  highest_topic  \n",
       "0  0.096  0.006              3  \n",
       "1  0.005  0.005              3  \n",
       "2  0.350  0.002              1  \n",
       "3  0.003  0.003              3  \n",
       "4  0.136  0.003              3  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.61 s (started: 2021-10-17 20:09:13 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#Bueno, primero correremos el código anterior\n",
    "\n",
    "# generamos una transformación de los datos a distribución de tópico por palabra en el documento\n",
    "fit_best_lda = best_lda.transform(count_vectorizer_fit)\n",
    "\n",
    "# estra transformación la podemos coercionar a un dataframe de la siguiente manera:        \n",
    "topics_for_each_doc = pd.DataFrame(\n",
    "\n",
    "np.round(fit_best_lda, 3), # pasamos esta matriz y la redondeamos en 3 decimales\n",
    "index=df.index\n",
    ") # agregamos un índice\n",
    "\n",
    "# agregamos identificadores de columna\n",
    "topics_for_each_doc.columns = list(map(lambda x: \"T: {}\".format(x), range(1, best_lda.n_components + 1)))\n",
    "\n",
    "# concatenamos las probabilidades de tópico por documento a nuestra matriz original\n",
    "concatenated_df = pd.concat([df, topics_for_each_doc], axis=1)\n",
    "\n",
    "# argmax en la matriz de tópicos\n",
    "concatenated_df['highest_topic'] = np.argmax(concatenated_df[docs_topics].values, axis=1) + 1\n",
    "\n",
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3ajTA3eTKLn"
   },
   "source": [
    "## b) Con esta nueva base de datos, identifique las probabilidades de pertenencia para un artista específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1634488526937,
     "user": {
      "displayName": "Francisco Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnBCIyF5R8AiTi5ykZU8rl4YlVuugPZqCU5uBJ4YQ=s64",
      "userId": "14439525103954735478"
     },
     "user_tz": 180
    },
    "id": "syAhokPgTMHf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    0.597523\n",
       "2    0.321981\n",
       "5    0.052632\n",
       "1    0.015480\n",
       "3    0.012384\n",
       "Name: highest_topic, dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-10-17 20:18:49 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#Sólo por conveniencia, elegiré 'Britney Spears' (lol)\n",
    "graf = concatenated_df.query(\"Artista == 'Britney Spears'\")['highest_topic'].value_counts('%')\n",
    "graf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentario**\n",
    "\n",
    "Al parecer Britney Spears se asocia más al tópico 4 con un 59,7% y al tópico 2 con un 32,1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMO6NpBbTORp"
   },
   "source": [
    "## c) Grafique la distribución de las probabilidades para algún artista en específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1634488526938,
     "user": {
      "displayName": "Francisco Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnBCIyF5R8AiTi5ykZU8rl4YlVuugPZqCU5uBJ4YQ=s64",
      "userId": "14439525103954735478"
     },
     "user_tz": 180
    },
    "id": "OvdhiPBbTPC9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 93 ms (started: 2021-10-17 20:19:06 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#Seguimos con Britney Spears!\n",
    "import seaborn as sns\n",
    "sns.barplot(x = graf.index, y = graf.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FS - Desafío - Inferencia de tópicos con EM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
